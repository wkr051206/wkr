## 简要介绍
1. 搭骨架​​：所有神经网络都要继承 nn.Module
   class MyNet(nn.Module):
2. 建结构​​：在 __init__里定义网络层（如 nn.Linear, nn.Conv2d）
3. 定流向​​：在 forward方法中写数据如何流动
   def forward(self, x): 层1→层2→...→输出
4. ​​一键操作​​：
   •训练：输出 = model(输入)
   •移动：model.to("cuda")
   •保存：torch.save(model.state_dict())
![](media/Pasted%20image%2020250819224217.png)

## 骨架搭建四步法（附代码模板）
```python
import torch.nn as nn

# 1️⃣ 继承nn.Module基类
class YourModel(nn.Module):  

    # 2️⃣ 初始化网络结构（定义层）
    def __init__(self, input_size, output_size):
        super().__init__()  # 必须调用父类初始化！

        # 定义网络层（例如）
        self.layer1 = nn.Linear(input_size, 128)  # 全连接层
        self.layer2 = nn.ReLU()                   # 激活函数
        self.layer3 = nn.Linear(128, output_size) # 输出层

    # 3️⃣ 定义数据流向（前向传播）
    def forward(self, x):  
        x = self.layer1(x)
        x = self.layer2(x) 
        x = self.layer3(x)  
        return x

# 4️⃣ 实例化并使用
model = YourModel(input_size=784, output_size=10)
output = model(input_data)  # 自动调用forward()
```
![](media/Pasted%20image%2020250819224431.png)

## 关键技巧图解

1. 参数自动追踪原理
```python
# 只有继承nn.Module才会注册参数！
for param in model.parameters():
    print(param.shape)  # 自动收集所有可学习参数

# 输出示例：
# torch.Size([128, 784])  ← layer1.weight
# torch.Size([128])        ← layer1.bias
# torch.Size([10, 128])    ← layer3.weight
# ...
```
2. 子模块嵌套（复杂网络必备）
```python
class SuperNet(nn.Module):
    def __init__(self):
        super().__init__()
        # 包含子模型
        self.feature_extractor = YourModel(784, 256) 
        self.classifier = nn.Sequential(
            nn.Linear(256, 64),
            nn.ReLU(),
            nn.Linear(64, 10)
        )
    
    def forward(self, x):
        features = self.feature_extractor(x)
        return self.classifier(features)
```
3. 混合搭配现成模块
```python
# 使用官方预建层（推荐！）
model = nn.Sequential(
    nn.Conv2d(3, 16, kernel_size=3),  # 卷积层
    nn.BatchNorm2d(16),               # 批归一化
    nn.ReLU(),
    nn.MaxPool2d(2),                  # 池化
    nn.Flatten(),                     # 展平
    nn.Linear(16 * 14 * 14, 10)           # 全连接
)
```
