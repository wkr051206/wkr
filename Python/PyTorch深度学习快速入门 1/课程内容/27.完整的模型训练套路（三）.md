# PyTorch训练模式与评估模式深度解析

## 一、训练/评估模式核心机制

### 模式切换原理

```python
graph TD
    A[模型模式] --> B[训练模式 model.train()]
    A --> C[评估模式 model.eval()]
    B --> D[启用Dropout]
    B --> E[更新BatchNorm统计量]
    C --> F[禁用Dropout]
    C --> G[冻结BatchNorm统计量]
```

### 影响层类型对比表

|层类型|训练模式行为|评估模式行为|是否需要显式切换|
|---|---|---|---|
|​**​Dropout​**​|随机丢弃神经元|所有神经元激活|✅ 必须|
|​**​BatchNorm​**​|更新均值和方差|使用训练统计量|✅ 必须|
|​**​LayerNorm​**​|无变化|无变化|❌ 可选|
|​**​卷积层​**​|无变化|无变化|❌ 可选|
|​**​线性层​**​|无变化|无变化|❌ 可选|

## 二、完整训练流程最佳实践

### 标准训练模板

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader

# 1. 初始化组件
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = MyModel().to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)
train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=128)

# 2. 训练循环
for epoch in range(epochs):
    # === 训练阶段 ===
    model.train()  # 设置训练模式
    for inputs, labels in train_loader:
        inputs, labels = inputs.to(device), labels.to(device)
        
        # 前向传播
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        
        # 反向传播
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    
    # === 评估阶段 ===
    model.eval()  # 设置评估模式
    test_loss = 0.0
    correct = 0
    total = 0
    
    with torch.no_grad():  # 禁用梯度计算
        for inputs, labels in test_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            
            # 计算损失
            test_loss += criterion(outputs, labels).item()
            
            # 计算准确率
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()
    
    # 打印统计信息
    print(f"Epoch {epoch+1}/{epochs} | Test Loss: {test_loss/len(test_loader):.4f} | Acc: {100.*correct/total:.2f}%")
    
    # 保存模型
    torch.save(model.state_dict(), f"model_epoch{epoch+1}.pth")
```

## 三、模式切换的底层原理

### BatchNorm层行为差异

```python
# 训练模式
running_mean = (1 - momentum) * running_mean + momentum * batch_mean
running_var = (1 - momentum) * running_var + momentum * batch_var

# 评估模式
normalized = (input - running_mean) / torch.sqrt(running_var + eps)
```

### Dropout层行为差异

```python
# 训练模式
mask = (torch.rand(input.shape) > p) / (1 - p)  # 伯努利采样
output = input * mask

# 评估模式
output = input  # 直接输出
```

## 四、高级应用场景

### 混合模式训练

```python
# 部分层保持评估模式（冻结）
model.eval()  # 先设置全局评估模式
model.fc.train()  # 仅训练最后的全连接层

for param in model.parameters():
    param.requires_grad = False
model.fc.weight.requires_grad = True
model.fc.bias.requires_grad = True
```

### 模型推理优化

```python
# 转换为推理模式（PyTorch 1.9+）
model.eval()
model = torch.jit.optimize_for_inference(torch.jit.script(model))
```

## 五、常见问题解决方案

### 问题1：忘记切换模式

​**​现象​**​：评估时Dropout仍激活 → 结果随机波动

​**​解决​**​：在验证/测试前添加`model.eval()`，结束后恢复`model.train()`

### 问题2：BatchNorm统计量漂移

​**​现象​**​：小批量测试导致统计量不准确

​**​解决​**​：使用`model.eval()`冻结统计量或启用`track_running_stats=False`

### 问题3：梯度计算泄漏

​**​现象​**​：评估阶段未禁用梯度 → 内存占用高

​**​解决​**​：使用`with torch.no_grad()`上下文管理器

## 六、GPU加速训练集成

### 多设备训练配置

```python
# 单GPU
device = torch.device("cuda:0")
model.to(device)

# 多GPU
if torch.cuda.device_count() > 1:
    model = nn.DataParallel(model)
```

### 混合精度训练

```python
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

for inputs, labels in train_loader:
    inputs, labels = inputs.to(device), labels.to(device)
    
    with autocast():
        outputs = model(inputs)
        loss = criterion(outputs, labels)
    
    scaler.scale(loss).backward()
    scaler.step(optimizer)
    scaler.update()
```

## 七、模型保存与加载策略

### 推荐保存方式

```python
# 保存
torch.save({
    'epoch': epoch,
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
    'loss': loss,
}, 'checkpoint.pth')

# 加载
checkpoint = torch.load('checkpoint.pth')
model.load_state_dict(checkpoint['model_state_dict'])
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
epoch = checkpoint['epoch']
```

### 设备兼容处理

```python
# CPU加载GPU保存的模型
model.load_state_dict(torch.load('gpu_model.pth', map_location=torch.device('cpu')))
```

## 八、最佳实践总结

### 训练流程检查表

|步骤|关键操作|注意事项|
|---|---|---|
|​**​数据准备​**​|创建DataLoader|设置shuffle=True|
|​**​训练开始​**​|model.train()|启用Dropout/BN更新|
|​**​前向传播​**​|计算输出|-|
|​**​损失计算​**​|criterion(output, target)|-|
|​**​反向传播​**​|loss.backward()|先optimizer.zero_grad()|
|​**​参数更新​**​|optimizer.step()|-|
|​**​评估开始​**​|model.eval()|禁用Dropout/冻结BN|
|​**​评估计算​**​|with torch.no_grad()|禁用梯度计算|
|​**​模型保存​**​|torch.save(state_dict)|保存状态字典|

### 性能优化技巧

1. ​**​异步数据加载​**​：
    
    ```python
    DataLoader(..., num_workers=4, pin_memory=True)
    ```
    
2. ​**​梯度累积​**​：
    
    ```python
    # 每4步更新一次
    if (i+1) % 4 == 0:
        optimizer.step()
        optimizer.zero_grad()
    ```
    
3. ​**​自动混合精度​**​：
    
    ```python
    from torch.cuda.amp import autocast
    with autocast():
        outputs = model(inputs)
    ```
    

> ​**​终极建议​**​：对于简单模型（无Dropout/BN），可省略模式切换；对于复杂模型，严格遵循`model.train()`和`model.eval()`切换规范。使用`torch.no_grad()`在评估阶段始终推荐，可显著降低内存占用。