**核心功能：将数据集自动打包成可迭代的批次流**
支持：
1. 自动分批
2. 打乱顺序
3. 并行加载
4. 内存管理
## 基本使用模板
```python
from torch.utils.data import DataLoader

# 创建DataLoader (假设已有dataset对象)
loader = DataLoader(
    dataset,           # 你的数据集对象
    batch_size=32,      # 每批数据量
    shuffle=True,       # 是否打乱数据 (训练集用True，测试集用False)
    num_workers=4,       # 并行加载的进程数 (建议设为CPU核心数)
    pin_memory=True     # 启用锁页内存 (GPU训练必开)
)

# 遍历批数据
for batch in loader:
    # batch包含 (inputs, labels) 或按数据集定义的格式
    inputs, labels = batch[0].to(device), batch[1].to(device)
    # ... 训练/验证代码 ...****
```
![](media/Pasted%20image%2020250819223159.png)

## 高效使用技巧
1. 批处理可视化
```python
# 快速查看批数据形态 (调试神器)
batch = next(iter(loader))
print(f"输入形状: {batch[0].shape}")  # [batch,通道,高,宽]
print(f"标签形状: {batch[1].shape}")  # [batch]
```
2. 自定义批处理逻辑
```python
# 高级用法：自定义数据组装方式
def collate_fn(batch):
    # 特殊处理不规则数据 (如文本序列)
    inputs = [item[0] for item in batch]
    labels = torch.stack([item[1] for item in batch])
    return inputs, labels

loader = DataLoader(..., collate_fn=collate_fn)
```
3. 分布式训练支持
```python
# 多GPU训练必备
from torch.utils.data.distributed import DistributedSampler

sampler = DistributedSampler(dataset)
loader = DataLoader(
    dataset, 
    sampler=sampler,    # 替换shuffle参数
    batch_size=32,
    num_workers=4
)
```